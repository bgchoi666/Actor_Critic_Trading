{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jQ1tEQCxwRx"
   },
   "source": [
    "Copyright 2023 Bumghi Choi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:09.491374Z",
     "iopub.status.busy": "2022-12-14T22:10:09.490680Z",
     "iopub.status.idle": "2022-12-14T22:10:09.495192Z",
     "shell.execute_reply": "2022-12-14T22:10:09.494526Z"
    },
    "id": "V_sgB_5dx1f1"
   },
   "outputs": [],
   "source": [
    "# 이 프로그램은 코스피200선물 거래를 위해서 tensorflow 홈페이지에 있는 actor-critic sample을 기반으로 작성되었다.\n",
    "# 거래 수익률과 현재 보유 상태를 state로 하고 매수/매도 를  action으로 한다.\n",
    "# actor에 의한 거래 시그널이 과거 60분봉 선물지수 데이터에 의거하여 수익률과 보유 상태의 변화를 준다.\n",
    "# A2C (advantage actor-critic) 방식으로 구성하고 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p62G8M_viUJp"
   },
   "source": [
    "# Actor-Critic 메서드로 코스피200선물 거래하기 - DeepMoney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kA10ZKRR0hi"
   },
   "source": [
    "**Actor-Critic 방법**\n",
    "\n",
    "Actor-Critic 방법은 가치 함수와 독립적인 정책 함수를 나타내는 [Temporal Difference(TD) 학습](https://en.wikipedia.org/wiki/Temporal_difference_learning) 방법입니다.\n",
    "\n",
    "정책 함수(또는 정책)는 에이전트가 주어진 상태에 따라 취할 수 있는 동작에 대한 확률 분포를 반환합니다. 가치 함수는 주어진 상태에서 시작하여 특정 정책에 따라 영원히 동작하는 에이전트의 예상 이익을 결정합니다.\n",
    "\n",
    "Actor-Critic 방법에서 정책은 주어진 상태에 따라 가능한 일련의 동작을 제안하는 *행위자*라고 하며, 추정값 함수는 주어진 정책에 따라 *행위자*가 취한 동작을 평가하는 *비평가*라고 합니다.\n",
    "\n",
    "이 튜토리얼에서 *행위자*와 *비평가* 모두 두 개의 출력이 있는 하나의 신경망을 사용하여 표현됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBfiafKSRs2k"
   },
   "source": [
    "코스피 200 선물 - 60분봉 데이터 - actor에 의한 거래 시그널을 적용하여 거래한 결과 (예)\n",
    "\n",
    "* 0 : 중립,  1: 매도,   2: 매수\n",
    "\n",
    "date\t거래 시그널\t종가\t수익\t수수료\n",
    "2022/04/19/13:00\t1\t357.5\t0\t0\n",
    "2022/04/19/14:00\t1\t357.25\t0\t0\n",
    "2022/04/19/15:00\t2\t357.2\t75000\t5360.25\n",
    "2022/04/20/09:00\t0\t355.7\t0\t0\n",
    "2022/04/20/10:00\t2\t355.45\t0\t0\n",
    "2022/04/20/11:00\t0\t356\t0\t0\n",
    "2022/04/20/12:00\t0\t357.1\t0\t0\n",
    "2022/04/20/13:00\t0\t357.7\t0\t0\n",
    "2022/04/20/14:00\t0\t357.45\t500000\t5346.75\n",
    "2022/04/20/15:00\t2\t356.95\t0\t0\n",
    "2022/04/21/09:00\t0\t359.85\t0\t0\n",
    "2022/04/21/10:00\t2\t360.4\t0\t0\n",
    "2022/04/21/11:00\t2\t360.2\t0\t0\n",
    "2022/04/21/12:00\t0\t359.45\t0\t0\n",
    "2022/04/21/13:00\t0\t359.7\t0\t0\n",
    "2022/04/21/14:00\t0\t359.65\t-187500\t5400.375\n",
    "2022/04/21/15:00\t2\t358.9\t0\t0\n",
    "2022/04/22/09:00\t1\t353.75\t0\t0\n",
    "2022/04/22/10:00\t1\t354.6\t0\t0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSNVK0AeRoJd"
   },
   "source": [
    "이 문제는 거래 에피소드에 대한 평균 총 수익이 100분봉 거래에서 5* 250000에 도달하면 \"해결\"된 것으로 간주됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glLwIctHiUJq"
   },
   "source": [
    "## 설정\n",
    "\n",
    "필요한 패키지를 가져오고 전역 설정을 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:32.605806Z",
     "iopub.status.busy": "2022-12-14T22:10:32.605042Z",
     "iopub.status.idle": "2022-12-14T22:10:35.157910Z",
     "shell.execute_reply": "2022-12-14T22:10:35.157184Z"
    },
    "id": "tT4N3qYviUJr"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코스피200 선물 파일로부터 tran, test 를 위한 dataframe을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(\"kospi200f_60M.csv\", encoding=\"euc-kr\")\n",
    "train_df = df0.loc[df0['date'] <= '2021/12/31/15:00']\n",
    "train_df = train_df.loc[train_df['date'] >= '2017/01/01/09:00']\n",
    "\n",
    "test_df = df0.loc[df0['date'] >= '2022/01/01/09:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOUCe2D0iUJu"
   },
   "source": [
    "## 모델\n",
    "\n",
    "*행위자*와 *비평가*는 각각 동작 확률과 비평 값을 생성하는 하나의 신경망을 사용하여 모델링됩니다. 이 튜토리얼에서는 모델 하위 클래스화를 사용하여 모델을 정의합니다.\n",
    "\n",
    "순방향 전달 중에 모델은 상태를 입력으로 받고 상태 종속 값을 모델링하는 동작 확률과 비평 값 $V$를 모두 출력합니다. 목표는 예상 이익을 최대화하는 $\\pi$ 정책을 기반으로 행동을 선택하는 모델을 훈련하는 것입니다.\n",
    "\n",
    "deepmoney의 경우, 상태를 나타내는 4 가지 값이 있는데, 각각 20일 수익률(%), 5일 수익률(%), 1일 수익률(%) 및 보유 상태(0: 없음, 1:매도, 2: 매수)입니다. 에이전트는 중립(0, 거래없음), 매도(1, 청산후 진입)와 매수(2, 청산후 진입) 3가지 동작을 취할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:35.162501Z",
     "iopub.status.busy": "2022-12-14T22:10:35.161720Z",
     "iopub.status.idle": "2022-12-14T22:10:35.168332Z",
     "shell.execute_reply": "2022-12-14T22:10:35.167732Z"
    },
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x = self.common(inputs)\n",
    "    return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:35.171570Z",
     "iopub.status.busy": "2022-12-14T22:10:35.171102Z",
     "iopub.status.idle": "2022-12-14T22:10:38.812238Z",
     "shell.execute_reply": "2022-12-14T22:10:38.811428Z"
    },
    "id": "nWyxJgjLn68c"
   },
   "outputs": [],
   "source": [
    "num_actions = 3  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 거래 환경 제공 및 훈련 데이터 생성\n",
    "\n",
    "2022-01-01~ 2023-01-20 까지의 코스피200 선물 가격 데이터로부터 모델의 action(매수/매도 거래 시그널)과 현재 state에 의한 수익을 평가하고 5일, 20일 평균 수익을 계산하여 새로운 state를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class env():\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      mode: str): \n",
    "\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "\n",
    "    self.mode = mode\n",
    "    self.initial_pos = 0\n",
    "    self.current_pos = 0\n",
    "    self.train_df = train_df\n",
    "    self.test_df = test_df\n",
    "    if self.mode == 'train':\n",
    "        self.price = self.train_df['종가'].values\n",
    "    else:\n",
    "        self.price = self.test_df['종가'].values\n",
    "    self.state = tf.constant([0.0,0.0,0.0,0.0,0.0,0], dtype=tf.float32)\n",
    "\n",
    "  def step(self, action: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "\n",
    "    if self.mode == 'train' and self.current_pos > len(self.train_df) - 2:\n",
    "        return self.state, tf.constant(0)\n",
    "    elif self.mode == 'test' and self.current_pos > len(self.test_df) - 2:\n",
    "        return self.state, tf.constant(0)\n",
    "    \n",
    "    # state(일자별 수익률) 한칸씩 왼쪽으로 이동\n",
    "    new_state  = np.zeros(6)\n",
    "    for i in range(5):\n",
    "        new_state[i] = self.state[i+1]\n",
    "        \n",
    "  \n",
    "    # action에 의한 보유 상태 변경\n",
    "    if action == 1:\n",
    "        new_state[5] = max(self.state[5] - 1, -1)\n",
    "    elif action == 2:\n",
    "        new_state[5] = min(self.state[5] + 1, 1)    \n",
    "    \n",
    "    # 변경된 보유상태에서 가격 변동에 따른 마지막 step의 수익률 변경\n",
    "    if new_state[5] == 0:\n",
    "        new_state[4] = 0\n",
    "    elif new_state[5] == 1:\n",
    "        new_state[4] = (self.price[self.current_pos+1] - self.price[self.current_pos]) / self.price[i]\n",
    "    else:\n",
    "        new_state[4] = (self.price[self.current_pos] - self.price[self.current_pos+1]) / self.price[i]\n",
    "\n",
    "    # 손익 여부(-1, 0, 1)를 나타내는 rewrad 산출\n",
    "    reward = tf.sign(new_state[4])\n",
    "    \n",
    "    # 현재 position 이동\n",
    "    self.current_pos += 1\n",
    "\n",
    "    # 변경된 state 저장\n",
    "    self.state = tf.convert_to_tensor(new_state)\n",
    "    \n",
    "    return self.state, reward\n",
    "\n",
    "  def reset(self) -> tf.Tensor:\n",
    "    self.sate = tf.constant([0.0,0.0,0.0,0.0,0.0,0], dtype=tf.float32)\n",
    "    self.current_pos = self.initial_pos\n",
    "    return self.state\n",
    "\n",
    "env = env(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk92njFziUJw"
   },
   "source": [
    "## 에이전트 훈련\n",
    "\n",
    "에이전트를 훈련하기 위해 다음 단계를 따릅니다.\n",
    "\n",
    "1. 환경에서 에이전트를 실행하여 에피소드별로 훈련 데이터를 수집합니다.\n",
    "2. 각 시간 스텝에서 예상 이익을 계산합니다.\n",
    "3. 결합된 Actor-Critic 모델의 손실을 계산합니다.\n",
    "4. 그래디언트를 계산하고 네트워크 매개변수를 업데이트합니다.\n",
    "5. 성공 기준 또는 최대 에피소드에 도달할 때까지 1~4를 반복합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2nde2XDs8Gh"
   },
   "source": [
    "### 1. 훈련 데이터 수집\n",
    "\n",
    "지도 학습에서와 같이 Actor-Critic 모델을 훈련하려면 훈련 데이터가 필요합니다. 그러나, 이러한 데이터를 수집하려면 모델이 환경에서 \"실행\"되어야 합니다.\n",
    "\n",
    "여기서는 각 에피소드에 대한 훈련 데이터를 수집합니다. 그런 다음, 모델의 가중치에 의해 매개변수화된 현재 정책을 기반으로 동작 확률과 비평 값을 생성하기 위해 각 타임스텝에서 모델의 순방향 전달을 환경 상태에서 실행합니다.\n",
    "\n",
    "다음 동작은 모델에 의해 생성된 동작 확률로부터 샘플링되며, 그런 다음 환경에 적용되어 다음 상태와 보상을 생성합니다.\n",
    "\n",
    "이 프로세스는 더 빠른 훈련을 위해 나중에 TensorFlow 그래프로 컴파일할 수 있도록 TensorFlow 연산을 사용하는 `run_episode` 함수에서 구현됩니다. `tf.TensorArray`는 가변 길이 배열에서 Tensor 반복을 지원하는 데 사용되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.816930Z",
     "iopub.status.busy": "2022-12-14T22:10:38.816352Z",
     "iopub.status.idle": "2022-12-14T22:10:38.821816Z",
     "shell.execute_reply": "2022-12-14T22:10:38.821147Z"
    },
    "id": "5URrbGlDSAGx"
   },
   "outputs": [],
   "source": [
    "# initiate env to apply a step of action\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward = env.step(action)\n",
    "  return (np.array(state, np.float32), \n",
    "          np.array(reward, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], \n",
    "                           [tf.float32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "  \n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state)\n",
    "  \n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "  \n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "  \n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "  \n",
    "  return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "a, c, c = run_episode(state, model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBnIHdz22dIx"
   },
   "source": [
    "### 2. 예상 이익 계산\n",
    "\n",
    "한 에피소드 동안 수집된 각 타임스텝 $t$, ${r_{t}}^{T}*{t=1}$에서 보상의 시퀀스를 예상 이익 ${G*{t}}^{T}_{t=1}$의 시퀀스로 변환합니다. 여기서 보상의 합계는 현재 타임스텝 $t$에서 $T$까지 계산되며, 각 보상에 기하급수적으로 감소하는 할인 계수 $\\gamma$를 곱합니다.\n",
    "\n",
    "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "$\\gamma\\in(0,1)$ 이후, 현재 타임스텝에서 더 멀리 떨어진 보상에는 더 적은 가중치가 부여됩니다.\n",
    "\n",
    "직관적으로, 예상 이익은 단순히 지금 보상이 이후 보상보다 낫다는 것을 암시합니다. 이것은 수학적 의미에서 보상의 합이 수렴하도록 하려는 것입니다.\n",
    "\n",
    "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.835967Z",
     "iopub.status.busy": "2022-12-14T22:10:38.835427Z",
     "iopub.status.idle": "2022-12-14T22:10:38.841321Z",
     "shell.execute_reply": "2022-12-14T22:10:38.840630Z"
    },
    "id": "jpEwFyl315dl"
   },
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.00088585  0.         -0.00113895 -0.00063275  0.          0.        ], shape=(6,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[-2.48302    -1.2427393   0.01006951 -1.2361646   0.01671065  0.02639236\n",
      "  1.2920207   0.05873558  0.06884184  1.334899    1.3578957   0.12527598\n",
      "  1.391903    1.4154757   0.18343768  0.19480352  0.20628415 -1.0379679\n",
      "  0.21690923  0.22861314  0.24043532  1.5082256   0.2771244   0.28943658\n",
      "  0.3018731  -0.94141346 -2.1972585  -0.9540912  -0.9542155   0.3015076 ], shape=(30,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0.3333217  0.3429361  0.34298012 0.34285575 0.35848567 0.3332165\n",
      " 0.3608474  0.3023004  0.33345374 0.3022401  0.33353767 0.34282923\n",
      " 0.35851708 0.33329335 0.3022389  0.3333925  0.33333188 0.36091852\n",
      " 0.33662593 0.33330247 0.33331957 0.36094564 0.33676463 0.33340073\n",
      " 0.33324492 0.36084527 0.36102754 0.33643493 0.33394197 0.34286952], shape=(30,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())\n",
    "a, b, rewards = run_episode(env.state, model, 30)\n",
    "print(get_expected_return(rewards, 0.99, True))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhr50_Czxazw"
   },
   "source": [
    "### 3. Actor-Critic 손실\n",
    "\n",
    "하이브리드 Actor-Critic 모델을 사용하고 있기 때문에, 아래와 같이 훈련을 위해 Actor와 Critic 손실의 조합인 손실 함수를 사용합니다.\n",
    "\n",
    "$$L = L_{actor} + L_{critic}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOQIJuG1xdTH"
   },
   "source": [
    "#### Actor 손실\n",
    "\n",
    "[비평가가 상태 종속 기준선인 정책 그래디언트](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s)를 기반으로 행위자 손실을 공식화하고 단일 샘플(에피소드별) 추정치를 계산합니다.\n",
    "\n",
    "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
    "\n",
    "여기서:\n",
    "\n",
    "- $T$: 에피소드별로 달라질 수 있는 에피소드별 타임스텝의 수\n",
    "- $s_{t}$: $t$ 타임스텝의 상태\n",
    "- $a_{t}$: $s$ 상태에 따라 $t$ 타임스텝에서 선택된 동작\n",
    "- $\\pi_{\\theta}$: $\\theta$에 의해 매개변수화된 정책(Actor)\n",
    "- $V^{\\pi}_{\\theta}$: 마찬가지로 $\\theta$에 의해 매개변수화된 값 함수(Critic)\n",
    "- $G = G_{t}$: 주어진 상태에 대한 예상 이익, 타임스텝 $t$에서 동작 쌍\n",
    "\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y304O4OAxiAv"
   },
   "source": [
    "##### 이점\n",
    "\n",
    "$L_{actor}$ 공식에서 $G - V$ 항을 [이점](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions)이라고 하며, 이는 특정한 상태에서 $\\pi$ 정책에 따라 선택된 임의의 동작보다 이 상태에 얼마나 더 나은 동작이 주어지는지를 나타냅니다.\n",
    "\n",
    "기준선을 제외할 수 있지만 이로 인해 훈련 중에 큰 변동이 발생할 수 있습니다. 그리고 비평가 $V$를 기준선으로 선택할 때의 좋은 점은 가능한 한 $G$에 가깝게 훈련되어 변동이 낮아진다는 것입니다.\n",
    "\n",
    "또한, Critic이 없으면 알고리즘이 예상 이익을 바탕으로 특정 상태에서 취하는 행동의 확률을 높이려고 시도할 것이며, 이 때 동작 사이의 상대적 확률이 같게 유지된다면 큰 차이가 생기지 않습니다.\n",
    "\n",
    "예를 들어, 주어진 상태에서 두 행동의 예상 이익이 같다고 가정합니다. Critic이 없으면 알고리즘은 목표 $J$에 따라 이들 동작의 확률을 높이려고 합니다. Critic의 경우, 이점($G - V = 0$)이 없기 때문에 동작의 확률을 높이는 데 따른 이점이 없으며 알고리즘이 그래디언트를 0으로 설정합니다.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hrPLrgGxlvb"
   },
   "source": [
    "#### The Critic loss\n",
    "\n",
    "$V$를 $G$에 최대한 가깝게 훈련하는 것은 다음 손실 함수를 사용한 회귀 문제로 설정할 수 있습니다.\n",
    "\n",
    "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
    "\n",
    "여기서 $L_{\\delta}$는 [Huber 손실](https://en.wikipedia.org/wiki/Huber_loss)로, 제곱 오차 손실보다 데이터의 이상 값에 덜 민감합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.845345Z",
     "iopub.status.busy": "2022-12-14T22:10:38.844751Z",
     "iopub.status.idle": "2022-12-14T22:10:38.849607Z",
     "shell.execute_reply": "2022-12-14T22:10:38.848967Z"
    },
    "id": "9EXwbEez6n9m"
   },
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSYkQOmRfV75"
   },
   "source": [
    "### 4. 매개변수를 업데이트하기 위한 훈련 단계 정의\n",
    "\n",
    "위의 모든 단계를 모든 에피소드에서 실행되는 훈련 단계로 결합합니다. 손실 함수로 이어지는 모든 단계는 `tf.GradientTape` 컨텍스트로 실행되어 자동 미분이 가능합니다.\n",
    "\n",
    "이 튜토리얼에서는 Adam 옵티마이저를 사용하여 모델 매개변수에 그래디언트를 적용합니다.\n",
    "\n",
    "할인되지 않은 보상의 합계인 `episode_reward`도 이 단계에서 계산됩니다. 이 값은 나중에 성공 기준이 충족되는지 평가하는 데 사용됩니다.\n",
    "\n",
    "`tf.function` 컨텍스트를 `train_step` 함수에 적용하여 호출 가능한 TensorFlow 그래프로 컴파일할 수 있고, 그러면 훈련 속도가 10배 빨라질 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.852965Z",
     "iopub.status.busy": "2022-12-14T22:10:38.852688Z",
     "iopub.status.idle": "2022-12-14T22:10:38.864450Z",
     "shell.execute_reply": "2022-12-14T22:10:38.863749Z"
    },
    "id": "QoccrkF3IFCg"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode) \n",
    "\n",
    "    # Calculate the expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "    # Calculate the loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 2.78410529e-03 -1.29081245e-02  0.00000000e+00 -1.89825361e-03\n",
      "  5.06200962e-04  1.00000000e+00], shape=(6,), dtype=float64)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())\n",
    "print(env.current_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFvZiDoAflGK"
   },
   "source": [
    "### 5. 훈련 루프 실행하기\n",
    "\n",
    "성공 기준 또는 최대 에피소드 수에 도달할 때까지 훈련 단계를 실행하는 방식으로 훈련을 실행합니다.\n",
    "\n",
    "대기열을 사용하여 에피소드 보상의 실행 레코드를 유지합니다. 100회 시도에 도달하면 가장 오래된 보상이 대기열의 왼쪽(꼬리쪽) 끝에서 제거되고 최근 보상이 머리쪽(오른쪽)에 추가됩니다. 계산 효율을 높이기 위해 보상의 누적 합계도 유지됩니다.\n",
    "\n",
    "런타임에 따라 훈련은 1분 이내에 완료될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.868193Z",
     "iopub.status.busy": "2022-12-14T22:10:38.867630Z",
     "iopub.status.idle": "2022-12-14T22:17:13.392893Z",
     "shell.execute_reply": "2022-12-14T22:17:13.391873Z"
    },
    "id": "kbmBxnzLiUJx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1000/1000 [34:28<00:00,  2.07s/it, episode_reward=-1, running_reward=6.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 999: average reward: 6.67!\n",
      "Wall time: 34min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 1000\n",
    "max_steps_per_episode = 500 #100 #500\n",
    "\n",
    "# `CartPole-v1` is considered solved if average reward is >= 475 over 500 \n",
    "# consecutive trials\n",
    "reward_threshold = 475\n",
    "running_reward = 0\n",
    "\n",
    "# The discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep the last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "t = tqdm.trange(max_episodes)\n",
    "for i in t:\n",
    "    initial_state = env.reset()\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float64)\n",
    "    episode_reward = int(train_step(\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "    \n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "  \n",
    "\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "  \n",
    "    # Show the average episode reward every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "  \n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.825304Z",
     "iopub.status.busy": "2022-12-14T22:10:38.824691Z",
     "iopub.status.idle": "2022-12-14T22:10:38.832122Z",
     "shell.execute_reply": "2022-12-14T22:10:38.831465Z"
    },
    "id": "a4qVRV063Cl9"
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "    actions = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        # Convert state into a batched tensor (batch size = 1)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "\n",
    "        # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "\n",
    "\n",
    "        # Store actions\n",
    "        actions = actions.write(t, action)\n",
    "\n",
    "\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "\n",
    "    actions = actions.stack()\n",
    "  \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mode = 'test'\n",
    "initial_state = env.reset()\n",
    "initial_state = tf.constant(initial_state, dtype=tf.float64)\n",
    "\n",
    "env.test_df = test_df[:100]\n",
    "env.price = env.test_df['종가'].values\n",
    "\n",
    "max_steps = len(env.price) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3952532734198648\n"
     ]
    }
   ],
   "source": [
    "actions = test(initial_state, model, max_steps)\n",
    "\n",
    "# 15시 종가를 익일 시가로 조정\n",
    "#for i in range(len(df)-1):\n",
    "#    if i != 0 and df.loc[i, 'date'][11:13] == '15':\n",
    "#        df.loc[i, 'close'] = df.loc[i+1, 'open']\n",
    "\n",
    "state = 0\n",
    "count = 0\n",
    "buy_price = 0\n",
    "profit = []\n",
    "fee = []\n",
    "\n",
    "loss_cut = 0.01\n",
    "pred_term = 5\n",
    "\n",
    "for i in range(len(actions) - 1):\n",
    "\n",
    "    pred = actions[i]\n",
    "    close = float(test_df['종가'].values[i])\n",
    "    high = float(test_df['고가'].values[i])\n",
    "    low = float(test_df['저가'].values[i])\n",
    "    open = float(test_df['시가'].values[i])\n",
    "    date = test_df['date'].values[i]\n",
    "\n",
    "    if date[11:13] == '15':\n",
    "        if state == 1:\n",
    "            profit.append((buy_price - close) * 250000)\n",
    "            fee.append((buy_price + close) * 250000 * 0.00003)\n",
    "        elif state == 2:\n",
    "            profit.append((close - buy_price) * 250000)\n",
    "            fee.append((buy_price + close) * 250000 * 0.00003)\n",
    "        else:\n",
    "            profit.append(0)\n",
    "            fee.append(0)\n",
    "\n",
    "        state = 0\n",
    "        count = 0\n",
    "        buy_price = 0\n",
    "        \n",
    "        continue\n",
    "\n",
    "    if state == 1:\n",
    "\n",
    "        if high - buy_price >= buy_price * loss_cut:\n",
    "            profit.append(-int((buy_price*loss_cut+0.05)/0.05)*0.05*250000)\n",
    "            fee.append((buy_price + close)*250000*0.00003)\n",
    "            state = pred\n",
    "            count = 1\n",
    "            buy_price = close\n",
    "        elif pred == 2:\n",
    "            profit.append((buy_price - close)*250000)\n",
    "            fee.append((buy_price + close)*250000*0.00003)\n",
    "            state = pred\n",
    "            count = 1\n",
    "            buy_price = close\n",
    "        else:\n",
    "            profit.append(0)\n",
    "            fee.append(0)\n",
    "            count += 1\n",
    "            count %= pred_term\n",
    "    elif state == 2:\n",
    "\n",
    "        if buy_price - low >= buy_price * loss_cut:\n",
    "            profit.append(-int((buy_price*loss_cut+0.05)/0.05)*0.05*250000)\n",
    "            fee.append((buy_price + close)*250000*0.00003)\n",
    "            state = pred\n",
    "            count = 1\n",
    "            buy_price = close\n",
    "        elif pred == 1:\n",
    "            profit.append((close - buy_price)*250000)\n",
    "            fee.append((buy_price + close)*250000*0.00003)\n",
    "            state = pred\n",
    "            count = 1\n",
    "            buy_price = close\n",
    "        else:\n",
    "            profit.append(0)\n",
    "            fee.append(0)\n",
    "            count += 1\n",
    "            count %= pred_term\n",
    "    else:\n",
    "        if pred == 1 or pred == 2:\n",
    "            state = pred\n",
    "            count = 1\n",
    "            buy_price = close\n",
    "        else:\n",
    "            count = 0\n",
    "        profit.append(0)\n",
    "        fee.append(0)\n",
    "\n",
    "print((sum(profit) - sum(fee))/test_df['종가'].values.mean()/1.25/250000/0.075 + 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnq9Hzo1Po6X"
   },
   "source": [
    "## 다음 단계\n",
    "\n",
    "이 튜토리얼에서는 Tensorflow를 사용하여 Actor-Critic 방법을 구현하는 방법을 보여주었습니다.\n",
    "\n",
    "다음 단계로 Gym의 다른 환경에서 모델의 훈련을 시도할 수 있습니다.\n",
    "\n",
    "Actor-Critic 방법 및 Cartpole-v0 문제에 대한 추가 정보는 다음 리소스를 참조하세요.\n",
    "\n",
    "- [Actor-Critic 메서드](https://hal.inria.fr/hal-00840470/document)\n",
    "- [Actor-Critic 강의(CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
    "- [Cart Pole 학습 제어 문제 [Barto 등 1983]{/a}](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
    "\n",
    "TensorFlow에서 더 많은 강화 학습 예를 보려면 다음 리소스를 확인하세요.\n",
    "\n",
    "- [강화 학습 코드 예제(keras.io)](https://keras.io/examples/rl/)\n",
    "- [TF-Agents 강화 학습 라이브러리](https://www.tensorflow.org/agents)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
