{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jQ1tEQCxwRx"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:09.491374Z",
     "iopub.status.busy": "2022-12-14T22:10:09.490680Z",
     "iopub.status.idle": "2022-12-14T22:10:09.495192Z",
     "shell.execute_reply": "2022-12-14T22:10:09.494526Z"
    },
    "id": "V_sgB_5dx1f1"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p62G8M_viUJp"
   },
   "source": [
    "# Actor-Critic 메서드로 CartPole 문제 풀기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mJ2i6jvZ3sK"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFgN7h_wiUJq"
   },
   "source": [
    "이 튜토리얼에서는 TensorFlow로 [Actor-Critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) 메서드를 구현하여 [Open AI Gym](https://gym.openai.com/) [`CartPole-v0`](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) 환경에서 에이전트를 훈련하는 방법을 보여줍니다. 독자가 [(심층) 강화 학습](https://en.wikipedia.org/wiki/Deep_reinforcement_learning)의 [정책 그래디언트 메서드](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)에 어느 정도 익숙하다고 가정합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kA10ZKRR0hi"
   },
   "source": [
    "**Actor-Critic 방법**\n",
    "\n",
    "Actor-Critic 방법은 가치 함수와 독립적인 정책 함수를 나타내는 [Temporal Difference(TD) 학습](https://en.wikipedia.org/wiki/Temporal_difference_learning) 방법입니다.\n",
    "\n",
    "정책 함수(또는 정책)는 에이전트가 주어진 상태에 따라 취할 수 있는 동작에 대한 확률 분포를 반환합니다. 가치 함수는 주어진 상태에서 시작하여 특정 정책에 따라 영원히 동작하는 에이전트의 예상 이익을 결정합니다.\n",
    "\n",
    "Actor-Critic 방법에서 정책은 주어진 상태에 따라 가능한 일련의 동작을 제안하는 *행위자*라고 하며, 추정값 함수는 주어진 정책에 따라 *행위자*가 취한 동작을 평가하는 *비평가*라고 합니다.\n",
    "\n",
    "이 튜토리얼에서 *행위자*와 *비평가* 모두 두 개의 출력이 있는 하나의 신경망을 사용하여 표현됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBfiafKSRs2k"
   },
   "source": [
    "**`CartPole-v0`**\n",
    "\n",
    "[`CartPole-v0` 환경](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)에서는 마찰이 없는 트랙을 따라 이동하는 카트에 막대가 연결되어 있습니다. 막대는 똑바른 상태에서 시작되고 에이전트의 목표는 카트에 `-1` 또는 `+1`의 힘을 가하여 카트가 넘어지는 것을 방지하는 것입니다. 막대가 똑바로 유지될 때마다 `+1`의 보상이 주어집니다. 에피소드는 (1) 막대가 수직에서 15도 이상 기울어지거나 (2) 카트가 중앙에서 2.4 단위 이상 이동하면 끝납니다.\n",
    "\n",
    "<center>\n",
    "  <pre data-md-type=\"custom_pre\">&lt;figure&gt;\n",
    "    &lt;image src=\"https://tensorflow.org/tutorials/reinforcement_learning/images/cartpole-v0.gif\"&gt;\n",
    "    &lt;figcaption&gt;\n",
    "      Trained actor-critic model in Cartpole-v0 environment\n",
    "    &lt;/figcaption&gt;\n",
    "  &lt;/figure&gt;</pre>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSNVK0AeRoJd"
   },
   "source": [
    "이 문제는 에피소드에 대한 평균 총 보상이 100회 연속 시도에서 195에 도달하면 \"해결\"된 것으로 간주됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glLwIctHiUJq"
   },
   "source": [
    "## 설정\n",
    "\n",
    "필요한 패키지를 가져오고 전역 설정을 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:09.498759Z",
     "iopub.status.busy": "2022-12-14T22:10:09.498519Z",
     "iopub.status.idle": "2022-12-14T22:10:19.530973Z",
     "shell.execute_reply": "2022-12-14T22:10:19.529821Z"
    },
    "id": "13l6BbxKhCKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control] in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from gym[classic_control]) (6.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from gym[classic_control]) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from gym[classic_control]) (1.21.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from gym[classic_control]) (2.2.1)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from gym[classic_control]) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (4.4.0)\n",
      "Requirement already satisfied: pyglet in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (2.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[classic_control]\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:19.535571Z",
     "iopub.status.busy": "2022-12-14T22:10:19.534968Z",
     "iopub.status.idle": "2022-12-14T22:10:32.600363Z",
     "shell.execute_reply": "2022-12-14T22:10:32.599274Z"
    },
    "id": "WBeQhPi2S4m5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/docs\n",
      "  Cloning https://github.com/tensorflow/docs to c:\\users\\qhedge\\appdata\\local\\temp\\pip-req-build-enhp4w5d\n",
      "  Resolved https://github.com/tensorflow/docs to commit 7d9aab3abb979d304e768df250b7fd069d60497e\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: astor in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from tensorflow-docs==0.0.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from tensorflow-docs==0.0.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from tensorflow-docs==0.0.0.dev0) (5.7.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.12.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from tensorflow-docs==0.0.0.dev0) (3.19.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from tensorflow-docs==0.0.0.dev0) (6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (4.12.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (4.17.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (6.0.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (5.8.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from importlib-metadata>=3.6->nbformat->tensorflow-docs==0.0.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from importlib-metadata>=3.6->nbformat->tensorflow-docs==0.0.0.dev0) (3.11.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (22.2.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (5.10.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\qhedge\\anaconda3\\envs\\tensorflow2.11\\lib\\site-packages (from jupyter-core->nbformat->tensorflow-docs==0.0.0.dev0) (305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs 'C:\\Users\\qhedge\\AppData\\Local\\Temp\\pip-req-build-enhp4w5d'\n"
     ]
    }
   ],
   "source": [
    "#%%bash\n",
    "# Install additional packages for visualization\n",
    "#sudo apt-get install -y python-opengl > /dev/null 2>&1\n",
    "#pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
    "!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:32.605806Z",
     "iopub.status.busy": "2022-12-14T22:10:32.605042Z",
     "iopub.status.idle": "2022-12-14T22:10:35.157910Z",
     "shell.execute_reply": "2022-12-14T22:10:35.157184Z"
    },
    "id": "tT4N3qYviUJr"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOUCe2D0iUJu"
   },
   "source": [
    "## 모델\n",
    "\n",
    "*행위자*와 *비평가*는 각각 동작 확률과 비평 값을 생성하는 하나의 신경망을 사용하여 모델링됩니다. 이 튜토리얼에서는 모델 하위 클래스화를 사용하여 모델을 정의합니다.\n",
    "\n",
    "순방향 전달 중에 모델은 상태를 입력으로 받고 상태 종속 [값 함수](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions)를 모델링하는 동작 확률과 비평 값 $V$를 모두 출력합니다. 목표는 예상 [이익](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return)을 최대화하는 $\\pi$ 정책을 기반으로 행동을 선택하는 모델을 훈련하는 것입니다.\n",
    "\n",
    "`CartPole-v0`의 경우, 상태를 나타내는 네 가지 값이 있는데, 각각 카트 위치, 카트 속도, 막대 각도 및 막대 속도입니다. 에이전트는 카트를 각각 왼쪽(`0`)과 오른쪽(`1`)으로 밀기 위해 두 가지 동작을 취할 수 있습니다.\n",
    "\n",
    "더 자세한 내용은 [Gym의 Cart Pole 문서화 페이지](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) 및 Barto, Sutton 및 Anderson(1983)의 [*어려운 학습 제어 문제를 해결할 수 있는 뉴런 유사 적응 요소*](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)를 참조하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:35.162501Z",
     "iopub.status.busy": "2022-12-14T22:10:35.161720Z",
     "iopub.status.idle": "2022-12-14T22:10:35.168332Z",
     "shell.execute_reply": "2022-12-14T22:10:35.167732Z"
    },
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x = self.common(inputs)\n",
    "    return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:35.171570Z",
     "iopub.status.busy": "2022-12-14T22:10:35.171102Z",
     "iopub.status.idle": "2022-12-14T22:10:38.812238Z",
     "shell.execute_reply": "2022-12-14T22:10:38.811428Z"
    },
    "id": "nWyxJgjLn68c"
   },
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk92njFziUJw"
   },
   "source": [
    "## 에이전트 훈련\n",
    "\n",
    "에이전트를 훈련하기 위해 다음 단계를 따릅니다.\n",
    "\n",
    "1. 환경에서 에이전트를 실행하여 에피소드별로 훈련 데이터를 수집합니다.\n",
    "2. 각 시간 스텝에서 예상 이익을 계산합니다.\n",
    "3. 결합된 Actor-Critic 모델의 손실을 계산합니다.\n",
    "4. 그래디언트를 계산하고 네트워크 매개변수를 업데이트합니다.\n",
    "5. 성공 기준 또는 최대 에피소드에 도달할 때까지 1~4를 반복합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2nde2XDs8Gh"
   },
   "source": [
    "### 1. 훈련 데이터 수집\n",
    "\n",
    "지도 학습에서와 같이 Actor-Critic 모델을 훈련하려면 훈련 데이터가 필요합니다. 그러나, 이러한 데이터를 수집하려면 모델이 환경에서 \"실행\"되어야 합니다.\n",
    "\n",
    "여기서는 각 에피소드에 대한 훈련 데이터를 수집합니다. 그런 다음, 모델의 가중치에 의해 매개변수화된 현재 정책을 기반으로 동작 확률과 비평 값을 생성하기 위해 각 타임스텝에서 모델의 순방향 전달을 환경 상태에서 실행합니다.\n",
    "\n",
    "다음 동작은 모델에 의해 생성된 동작 확률로부터 샘플링되며, 그런 다음 환경에 적용되어 다음 상태와 보상을 생성합니다.\n",
    "\n",
    "이 프로세스는 더 빠른 훈련을 위해 나중에 TensorFlow 그래프로 컴파일할 수 있도록 TensorFlow 연산을 사용하는 `run_episode` 함수에서 구현됩니다. `tf.TensorArray`는 가변 길이 배열에서 Tensor 반복을 지원하는 데 사용되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.816930Z",
     "iopub.status.busy": "2022-12-14T22:10:38.816352Z",
     "iopub.status.idle": "2022-12-14T22:10:38.821816Z",
     "shell.execute_reply": "2022-12-14T22:10:38.821147Z"
    },
    "id": "5URrbGlDSAGx"
   },
   "outputs": [],
   "source": [
    "# Wrap Gym's `env.step` call as an operation in a TensorFlow function.\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward, done, truncated, info = env.step(action)\n",
    "  return (state.astype(np.float32), \n",
    "          np.array(reward, np.int32), \n",
    "          np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], \n",
    "                           [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.825304Z",
     "iopub.status.busy": "2022-12-14T22:10:38.824691Z",
     "iopub.status.idle": "2022-12-14T22:10:38.832122Z",
     "shell.execute_reply": "2022-12-14T22:10:38.831465Z"
    },
    "id": "a4qVRV063Cl9"
   },
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "  \n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state)\n",
    "  \n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "  \n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward, done = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "  \n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "    if tf.cast(done, tf.bool):\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "  \n",
    "  return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
      "array([0.4976292 , 0.4972296 , 0.502502  , 0.5071339 , 0.48551014,\n",
      "       0.5086761 , 0.51613027, 0.4760943 , 0.51889753, 0.47310778,\n",
      "       0.5221817 , 0.46961558, 0.47360453, 0.5217674 , 0.5302404 ],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
      "array([-0.00157411, -0.01534602,  0.00139814,  0.01624936,  0.02788474,\n",
      "        0.01461379,  0.02628089,  0.03787615,  0.02415822,  0.03675127,\n",
      "        0.02261917,  0.03553691,  0.02167694,  0.01373187,  0.02317774],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(15,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>)\n"
     ]
    }
   ],
   "source": [
    "initial_state, info = env.reset()\n",
    "initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "print(run_episode(initial_state, model, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBnIHdz22dIx"
   },
   "source": [
    "### 2. 예상 이익 계산\n",
    "\n",
    "한 에피소드 동안 수집된 각 타임스텝 $t$, ${r_{t}}^{T}*{t=1}$에서 보상의 시퀀스를 예상 이익 ${G*{t}}^{T}_{t=1}$의 시퀀스로 변환합니다. 여기서 보상의 합계는 현재 타임스텝 $t$에서 $T$까지 계산되며, 각 보상에 기하급수적으로 감소하는 할인 계수 $\\gamma$를 곱합니다.\n",
    "\n",
    "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "$\\gamma\\in(0,1)$ 이후, 현재 타임스텝에서 더 멀리 떨어진 보상에는 더 적은 가중치가 부여됩니다.\n",
    "\n",
    "직관적으로, 예상 이익은 단순히 지금 보상이 이후 보상보다 낫다는 것을 암시합니다. 이것은 수학적 의미에서 보상의 합이 수렴하도록 하려는 것입니다.\n",
    "\n",
    "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.835967Z",
     "iopub.status.busy": "2022-12-14T22:10:38.835427Z",
     "iopub.status.idle": "2022-12-14T22:10:38.841321Z",
     "shell.execute_reply": "2022-12-14T22:10:38.840630Z"
    },
    "id": "jpEwFyl315dl"
   },
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhr50_Czxazw"
   },
   "source": [
    "### 3. Actor-Critic 손실\n",
    "\n",
    "하이브리드 Actor-Critic 모델을 사용하고 있기 때문에, 아래와 같이 훈련을 위해 Actor와 Critic 손실의 조합인 손실 함수를 사용합니다.\n",
    "\n",
    "$$L = L_{actor} + L_{critic}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOQIJuG1xdTH"
   },
   "source": [
    "#### Actor 손실\n",
    "\n",
    "[비평가가 상태 종속 기준선인 정책 그래디언트](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s)를 기반으로 행위자 손실을 공식화하고 단일 샘플(에피소드별) 추정치를 계산합니다.\n",
    "\n",
    "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
    "\n",
    "여기서:\n",
    "\n",
    "- $T$: 에피소드별로 달라질 수 있는 에피소드별 타임스텝의 수\n",
    "- $s_{t}$: $t$ 타임스텝의 상태\n",
    "- $a_{t}$: $s$ 상태에 따라 $t$ 타임스텝에서 선택된 동작\n",
    "- $\\pi_{\\theta}$: $\\theta$에 의해 매개변수화된 정책(Actor)\n",
    "- $V^{\\pi}_{\\theta}$: 마찬가지로 $\\theta$에 의해 매개변수화된 값 함수(Critic)\n",
    "- $G = G_{t}$: 주어진 상태에 대한 예상 이익, 타임스텝 $t$에서 동작 쌍\n",
    "\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y304O4OAxiAv"
   },
   "source": [
    "##### 이점\n",
    "\n",
    "$L_{actor}$ 공식에서 $G - V$ 항을 [이점](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions)이라고 하며, 이는 특정한 상태에서 $\\pi$ 정책에 따라 선택된 임의의 동작보다 이 상태에 얼마나 더 나은 동작이 주어지는지를 나타냅니다.\n",
    "\n",
    "기준선을 제외할 수 있지만 이로 인해 훈련 중에 큰 변동이 발생할 수 있습니다. 그리고 비평가 $V$를 기준선으로 선택할 때의 좋은 점은 가능한 한 $G$에 가깝게 훈련되어 변동이 낮아진다는 것입니다.\n",
    "\n",
    "또한, Critic이 없으면 알고리즘이 예상 이익을 바탕으로 특정 상태에서 취하는 행동의 확률을 높이려고 시도할 것이며, 이 때 동작 사이의 상대적 확률이 같게 유지된다면 큰 차이가 생기지 않습니다.\n",
    "\n",
    "예를 들어, 주어진 상태에서 두 행동의 예상 이익이 같다고 가정합니다. Critic이 없으면 알고리즘은 목표 $J$에 따라 이들 동작의 확률을 높이려고 합니다. Critic의 경우, 이점($G - V = 0$)이 없기 때문에 동작의 확률을 높이는 데 따른 이점이 없으며 알고리즘이 그래디언트를 0으로 설정합니다.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hrPLrgGxlvb"
   },
   "source": [
    "#### The Critic loss\n",
    "\n",
    "$V$를 $G$에 최대한 가깝게 훈련하는 것은 다음 손실 함수를 사용한 회귀 문제로 설정할 수 있습니다.\n",
    "\n",
    "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
    "\n",
    "여기서 $L_{\\delta}$는 [Huber 손실](https://en.wikipedia.org/wiki/Huber_loss)로, 제곱 오차 손실보다 데이터의 이상 값에 덜 민감합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.845345Z",
     "iopub.status.busy": "2022-12-14T22:10:38.844751Z",
     "iopub.status.idle": "2022-12-14T22:10:38.849607Z",
     "shell.execute_reply": "2022-12-14T22:10:38.848967Z"
    },
    "id": "9EXwbEez6n9m"
   },
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSYkQOmRfV75"
   },
   "source": [
    "### 4. 매개변수를 업데이트하기 위한 훈련 단계 정의\n",
    "\n",
    "위의 모든 단계를 모든 에피소드에서 실행되는 훈련 단계로 결합합니다. 손실 함수로 이어지는 모든 단계는 `tf.GradientTape` 컨텍스트로 실행되어 자동 미분이 가능합니다.\n",
    "\n",
    "이 튜토리얼에서는 Adam 옵티마이저를 사용하여 모델 매개변수에 그래디언트를 적용합니다.\n",
    "\n",
    "할인되지 않은 보상의 합계인 `episode_reward`도 이 단계에서 계산됩니다. 이 값은 나중에 성공 기준이 충족되는지 평가하는 데 사용됩니다.\n",
    "\n",
    "`tf.function` 컨텍스트를 `train_step` 함수에 적용하여 호출 가능한 TensorFlow 그래프로 컴파일할 수 있고, 그러면 훈련 속도가 10배 빨라질 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.852965Z",
     "iopub.status.busy": "2022-12-14T22:10:38.852688Z",
     "iopub.status.idle": "2022-12-14T22:10:38.864450Z",
     "shell.execute_reply": "2022-12-14T22:10:38.863749Z"
    },
    "id": "QoccrkF3IFCg"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode) \n",
    "\n",
    "    # Calculate the expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "    # Calculate the loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFvZiDoAflGK"
   },
   "source": [
    "### 5. 훈련 루프 실행하기\n",
    "\n",
    "성공 기준 또는 최대 에피소드 수에 도달할 때까지 훈련 단계를 실행하는 방식으로 훈련을 실행합니다.\n",
    "\n",
    "대기열을 사용하여 에피소드 보상의 실행 레코드를 유지합니다. 100회 시도에 도달하면 가장 오래된 보상이 대기열의 왼쪽(꼬리쪽) 끝에서 제거되고 최근 보상이 머리쪽(오른쪽)에 추가됩니다. 계산 효율을 높이기 위해 보상의 누적 합계도 유지됩니다.\n",
    "\n",
    "런타임에 따라 훈련은 1분 이내에 완료될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.868193Z",
     "iopub.status.busy": "2022-12-14T22:10:38.867630Z",
     "iopub.status.idle": "2022-12-14T22:17:13.392893Z",
     "shell.execute_reply": "2022-12-14T22:17:13.391873Z"
    },
    "id": "kbmBxnzLiUJx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                   | 650/10000 [00:51<12:15, 12.70it/s, episode_reward=389, running_reward=477]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 650: average reward: 476.59!\n",
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "# `CartPole-v1` is considered solved if average reward is >= 475 over 500 \n",
    "# consecutive trials\n",
    "reward_threshold = 475\n",
    "running_reward = 0\n",
    "\n",
    "# The discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep the last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "t = tqdm.trange(max_episodes)\n",
    "for i in t:\n",
    "    initial_state, info = env.reset()\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "    episode_reward = int(train_step(\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "    \n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "  \n",
    "\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "  \n",
    "    # Show the average episode reward every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "  \n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru8BEwS1EmAv"
   },
   "source": [
    "## 시각화\n",
    "\n",
    "훈련 후에는 모델이 환경에서 어떻게 동작하는지 시각화하는 것이 좋습니다. 아래 셀을 실행하여 모델의 한 에피소드 실행에 대한 GIF 애니메이션을 생성할 수 있습니다. Colab에서 환경의 이미지를 올바르게 렌더링하려면 Gym에 대한 추가 패키지를 설치해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:17:13.397174Z",
     "iopub.status.busy": "2022-12-14T22:17:13.396375Z",
     "iopub.status.idle": "2022-12-14T22:17:16.003881Z",
     "shell.execute_reply": "2022-12-14T22:17:16.003080Z"
    },
    "id": "qbIMMkfmRHyC"
   },
   "outputs": [],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "\n",
    "render_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
    "  state, info = env.reset()\n",
    "  state = tf.constant(state, dtype=tf.float32)\n",
    "  screen = env.render()\n",
    "  images = [Image.fromarray(screen)]\n",
    " \n",
    "  for i in range(1, max_steps + 1):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_probs, _ = model(state)\n",
    "    action = np.argmax(np.squeeze(action_probs))\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "    # Render screen every 10 steps\n",
    "    if i % 10 == 0:\n",
    "      screen = env.render()\n",
    "      images.append(Image.fromarray(screen))\n",
    "  \n",
    "    if done:\n",
    "      break\n",
    "  \n",
    "  return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(render_env, model, max_steps_per_episode)\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:17:16.008021Z",
     "iopub.status.busy": "2022-12-14T22:17:16.007746Z",
     "iopub.status.idle": "2022-12-14T22:17:16.073664Z",
     "shell.execute_reply": "2022-12-14T22:17:16.072978Z"
    },
    "id": "TLd720SejKmf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/gif;base64,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\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnq9Hzo1Po6X"
   },
   "source": [
    "## 다음 단계\n",
    "\n",
    "이 튜토리얼에서는 Tensorflow를 사용하여 Actor-Critic 방법을 구현하는 방법을 보여주었습니다.\n",
    "\n",
    "다음 단계로 Gym의 다른 환경에서 모델의 훈련을 시도할 수 있습니다.\n",
    "\n",
    "Actor-Critic 방법 및 Cartpole-v0 문제에 대한 추가 정보는 다음 리소스를 참조하세요.\n",
    "\n",
    "- [Actor-Critic 메서드](https://hal.inria.fr/hal-00840470/document)\n",
    "- [Actor-Critic 강의(CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
    "- [Cart Pole 학습 제어 문제 [Barto 등 1983]{/a}](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
    "\n",
    "TensorFlow에서 더 많은 강화 학습 예를 보려면 다음 리소스를 확인하세요.\n",
    "\n",
    "- [강화 학습 코드 예제(keras.io)](https://keras.io/examples/rl/)\n",
    "- [TF-Agents 강화 학습 라이브러리](https://www.tensorflow.org/agents)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
