{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "_jQ1tEQCxwRx"
   },
   "source": [
    "Copyright 2024 Bumghi Choi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:09.491374Z",
     "iopub.status.busy": "2022-12-14T22:10:09.490680Z",
     "iopub.status.idle": "2022-12-14T22:10:09.495192Z",
     "shell.execute_reply": "2022-12-14T22:10:09.494526Z"
    },
    "id": "V_sgB_5dx1f1"
   },
   "source": [
    "# 프로그램 컨셉\n",
    "이 프로그램은 코스피200선물 거래를 위해서 tensorflow 홈페이지에 있는 A2C (advantage actor-critic) sample을 기반으로 작성되었다.\n",
    "\n",
    "이미 DNN으로 학습된 앙상블 모델의 포지션에서 actor-critic에 의해 산텍된 손절비율에 따른 수익률의 변화를 순차적으로 n개를 나열한 것과 현재 포지션을 state로 하여 actor와 critic의 공통 입력 데이터로 시용한다.\n",
    "\n",
    "actor의 output인 action은 거래 수량. 주어진 모델의 기대 수익이 critic의 목표값이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p62G8M_viUJp"
   },
   "source": [
    "# Actor-Critic 메서드로 코스피200선물 거래하기 - DeepMoney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kA10ZKRR0hi"
   },
   "source": [
    "**Actor-Critic 방법**\n",
    "\n",
    "Actor-Critic 방법은 가치 함수와 독립적인 정책 함수를 나타내는 [Temporal Difference(TD) 학습](https://en.wikipedia.org/wiki/Temporal_difference_learning) 방법입니다.\n",
    "\n",
    "정책 함수(또는 정책)는 에이전트가 주어진 상태에 따라 취할 수 있는 동작에 대한 확률 분포를 반환합니다. 가치 함수는 주어진 상태에서 시작하여 특정 정책에 따라 영원히 동작하는 에이전트의 예상 이익을 결정합니다.\n",
    "\n",
    "Actor-Critic 방법에서 정책은 주어진 상태에 따라 가능한 일련의 동작을 제안하는 *행위자*라고 하며, 추정값 함수는 주어진 정책에 따라 *행위자*가 취한 동작을 평가하는 *비평가*라고 합니다.\n",
    "\n",
    "이 튜토리얼에서 *행위자*와 *비평가* 모두 두 개의 출력이 있는 하나의 신경망을 사용하여 표현됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBfiafKSRs2k"
   },
   "source": [
    "딥러닝으로 이미 학습된 모델을 이용하여 시그널을 발생시킨다. 그 시그널에 의해 거래하는데 action은 손절 비율이 된다.\n",
    "코스피 200 선물 - 60분봉 데이터 - 딥러닝의 시그널과 actor의 거래량에 의해 거래한 결과 (예)\n",
    "\n",
    "* 0 : 중립,  1: 매도,   2: 매수\n",
    "\n",
    "date      거래 시그널\t종가\t수익\t수수료        거래량\n",
    "2022/04/19/13:00\t1\t357.5\t0\t0               1\n",
    "2022/04/19/14:00\t1\t357.25\t0\t0               0\n",
    "2022/04/19/15:00\t2\t357.2\t75000\t5360.25     2  \n",
    "2022/04/20/09:00\t0\t355.7\t0\t0               0\n",
    "2022/04/20/10:00\t2\t355.45\t0\t0               0\n",
    "2022/04/20/11:00\t0\t356\t0\t0                   0     \n",
    "2022/04/20/12:00\t0\t357.1\t0\t0\n",
    "2022/04/20/13:00\t0\t357.7\t0\t0\n",
    "2022/04/20/14:00\t0\t357.45\t500000\t5346.75\n",
    "2022/04/20/15:00\t2\t356.95\t0\t0\n",
    "2022/04/21/09:00\t0\t359.85\t0\t0\n",
    "2022/04/21/10:00\t2\t360.4\t0\t0\n",
    "2022/04/21/11:00\t2\t360.2\t0\t0\n",
    "2022/04/21/12:00\t0\t359.45\t0\t0\n",
    "2022/04/21/13:00\t0\t359.7\t0\t0\n",
    "2022/04/21/14:00\t0\t359.65\t-187500\t5400.375\n",
    "2022/04/21/15:00\t2\t358.9\t0\t0\n",
    "2022/04/22/09:00\t1\t353.75\t0\t0              2 \n",
    "2022/04/22/10:00\t1\t354.6\t0\t0              0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glLwIctHiUJq"
   },
   "source": [
    "## 설정\n",
    "\n",
    "필요한 패키지를 가져오고 전역 설정을 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:32.605806Z",
     "iopub.status.busy": "2022-12-14T22:10:32.605042Z",
     "iopub.status.idle": "2022-12-14T22:10:35.157910Z",
     "shell.execute_reply": "2022-12-14T22:10:35.157184Z"
    },
    "id": "tT4N3qYviUJr"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# 60분봉 데이터 load\n",
    "#df0 = pd.read_csv(\"kospi200f_60M.csv\", encoding='euc-kr')\n",
    "\n",
    "# 앙상블 모델의 거래 데이터 로드\n",
    "profit_df = pd.read_csv(\"ensemble_profits_2021~2024-05-10.csv\", encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = [\n",
    "[\"25HL\", \"30P\", \"20HL\", 0.284, 38, 14], # 알고리즘트레이딩2-1 2024-04-30 loss_cut 0.005\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "from data import config\n",
    "conf = config()\n",
    "import profit\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "conf.input_size = 83\n",
    "conf.df0_path = 'kospi200f_60M.csv'  # 원본 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코스피200 선물 파일 address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 파일, 정규화 파일(학습용, 예측 용), 예측 결과 파일\n",
    "\n",
    "conf.df0_path = 'kospi200f_60M2.csv'  # 원본 파일\n",
    "conf.result_path = 'pred_83_results.csv'  # 예측 결과 손익 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOUCe2D0iUJu"
   },
   "source": [
    "## 모델\n",
    "\n",
    "*행위자*와 *비평가*는 각각 동작 확률과 비평 값을 생성하는 하나의 신경망을 사용하여 모델링됩니다. 이 튜토리얼에서는 모델 하위 클래스화를 사용하여 모델을 정의합니다.\n",
    "\n",
    "순방향 전달 중에 모델은 상태를 입력으로 받고 상태 종속 값을 모델링하는 동작 확률과 비평 값 $V$를 모두 출력합니다. 목표는 예상 이익을 최대화하는 $\\pi$ 정책을 기반으로 행동을 선택하는 모델을 훈련하는 것입니다.\n",
    "\n",
    "상태는 35번의 누적수익률 list입니다. 에이전트는 손절비율유지(0, 손절비율유지), 손절비율 감소(1)와 손절비율 증가(2) 3가지 동작을 취할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:35.162501Z",
     "iopub.status.busy": "2022-12-14T22:10:35.161720Z",
     "iopub.status.idle": "2022-12-14T22:10:35.168332Z",
     "shell.execute_reply": "2022-12-14T22:10:35.167732Z"
    },
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.common1 = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.common2 = layers.Dense(int(num_hidden_units/2), activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x1 = self.common1(inputs)\n",
    "    x2 = self.common2(x1)\n",
    "    return self.actor(x2), self.critic(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:35.171570Z",
     "iopub.status.busy": "2022-12-14T22:10:35.171102Z",
     "iopub.status.idle": "2022-12-14T22:10:38.812238Z",
     "shell.execute_reply": "2022-12-14T22:10:38.811428Z"
    },
    "id": "nWyxJgjLn68c"
   },
   "outputs": [],
   "source": [
    "num_actions = 3 #(0: 거래량 유지, 1:거래량 감소, 2:거래량 증가)\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 거래 환경 제공 및 훈련 데이터 생성\n",
    "\n",
    "2022-01-01~ 2023-01-20 까지의 코스피200 선물 가격 데이터로부터 모델의 action(매수/매도 거래 시그널)과 현재 state에 의한 수익을 평가하고 5일, 20일 평균 수익을 계산하여 새로운 state를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class env:\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(self, conf): \n",
    "\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    self.conf = conf\n",
    "    self.start_time = conf.start_time\n",
    "    self.end_time = conf.end_time\n",
    "    self.loss_cut = 0.005\n",
    "    self.current_pos = int(35)\n",
    "    self.df = pd.read_csv(\"ensemble_profits_2021~2024-04-24.csv\", encoding='euc-kr')\n",
    "    self.state = tf.constant(np.ones(35), dtype=tf.float32)\n",
    "    \n",
    "  def step(self, action: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "\n",
    "    # new state 초기화\n",
    "    new_state  = np.roll(self.state, -1)\n",
    "       \n",
    "    # 현재 position 이동\n",
    "    self.current_pos += 1\n",
    "    self.current_pos = min(self.current_pos, len(self.df) - 1)\n",
    "    \n",
    "    # 거래량 증감에 따른 최근 수익률 증감 update\n",
    "    self.df.loc[self.current_pos-34:self.current_pos].to_csv(\"pred_83_results.csv\", encoding=\"euc-kr\")\n",
    "    if action == 1:\n",
    "        self.loss_cut = max(0, self.loss_cut - 0.001)\n",
    "        profit.loss_cut = self.loss_cut\n",
    "    elif action == 2:\n",
    "        self.loss_cut = min(0.01, self.loss_cut + 0.001)\n",
    "        profit.loss_cut = self.loss_cut\n",
    "    else:\n",
    "        profit.loss_cut = self.loss_cut\n",
    "    r, result_df = profit.calc_profit()\n",
    "    #result_df = pd.read_csv(\"pred_83_results.csv\", encoding=\"euc-kr\")\n",
    "\n",
    "    new_state[34] = new_state[33] + result_df['rate'].values[-1] - result_df['rate'].values[-2]\n",
    "        \n",
    "    reward = (result_df['rate'].values[-1] - result_df['rate'].values[-2]) * 100\n",
    "    \n",
    "    # 변경된 state 저장\n",
    "    self.state = tf.convert_to_tensor(new_state)\n",
    "    \n",
    "    return self.state, reward\n",
    "\n",
    "  def reset(self, conf) -> tf.Tensor:\n",
    "    self.conf = conf\n",
    "    self.start_time = conf.start_time\n",
    "    self.current_pos = self.df.loc[self.df['date'] >= self.start_time].index.min() + 34\n",
    "    self.df.loc[self.current_pos-34:self.current_pos].to_csv(\"pred_83_results.csv\", encoding=\"euc-kr\")\n",
    "    self.loss_cut = 0.005\n",
    "    profit.loss_cut = self.loss_cut\n",
    "    r, result_df = profit.calc_profit()\n",
    "    #result_df = pd.read_csv(\"pred_83_results.csv\", encoding=\"euc-kr\")\n",
    "    self.state = tf.convert_to_tensor(result_df['rate'].values)\n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(35,), dtype=float64, numpy=\n",
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.00649229, 1.00649229, 1.00649229, 1.00649229, 1.00649229,\n",
       "       1.00881662, 1.00881662, 1.00364114, 1.00364114, 1.00364114,\n",
       "       1.00364114, 1.00364114, 1.00364114, 1.00804886, 1.00995638,\n",
       "       1.00995638, 1.00995638, 1.00995638, 1.00995638, 1.01477948,\n",
       "       1.01477948, 1.02876853, 1.02876853, 1.02876853, 1.02876853,\n",
       "       1.02876853, 1.02192241, 1.00799275, 1.0257288 , 1.0257288 ])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk92njFziUJw"
   },
   "source": [
    "## 에이전트 훈련\n",
    "\n",
    "에이전트를 훈련하기 위해 다음 단계를 따릅니다.\n",
    "\n",
    "1. 환경에서 에이전트를 실행하여 에피소드별로 훈련 데이터를 수집합니다.\n",
    "2. 각 시간 스텝에서 예상 이익을 계산합니다.\n",
    "3. 결합된 Actor-Critic 모델의 손실을 계산합니다.\n",
    "4. 그래디언트를 계산하고 네트워크 매개변수를 업데이트합니다.\n",
    "5. 성공 기준 또는 최대 에피소드에 도달할 때까지 1~4를 반복합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2nde2XDs8Gh"
   },
   "source": [
    "### 1. 훈련 데이터 수집\n",
    "\n",
    "지도 학습에서와 같이 Actor-Critic 모델을 훈련하려면 훈련 데이터가 필요합니다. 그러나, 이러한 데이터를 수집하려면 모델이 환경에서 \"실행\"되어야 합니다.\n",
    "\n",
    "여기서는 각 에피소드에 대한 훈련 데이터를 수집합니다. 그런 다음, 모델의 가중치에 의해 매개변수화된 현재 정책을 기반으로 동작 확률과 비평 값을 생성하기 위해 각 타임스텝에서 모델의 순방향 전달을 환경 상태에서 실행합니다.\n",
    "\n",
    "다음 동작은 모델에 의해 생성된 동작 확률로부터 샘플링되며, 그런 다음 환경에 적용되어 다음 상태와 보상을 생성합니다.\n",
    "\n",
    "이 프로세스는 더 빠른 훈련을 위해 나중에 TensorFlow 그래프로 컴파일할 수 있도록 TensorFlow 연산을 사용하는 `run_episode` 함수에서 구현됩니다. `tf.TensorArray`는 가변 길이 배열에서 Tensor 반복을 지원하는 데 사용되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.816930Z",
     "iopub.status.busy": "2022-12-14T22:10:38.816352Z",
     "iopub.status.idle": "2022-12-14T22:10:38.821816Z",
     "shell.execute_reply": "2022-12-14T22:10:38.821147Z"
    },
    "id": "5URrbGlDSAGx"
   },
   "outputs": [],
   "source": [
    "# initiate env to apply a step of action\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward = env.step(action)\n",
    "  return (np.array(state, np.float32), \n",
    "          np.array(reward, np.float32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], \n",
    "                           [tf.float32, tf.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "  \n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state)\n",
    "  \n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "  \n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "  \n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "  \n",
    "  return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBnIHdz22dIx"
   },
   "source": [
    "### 2. 예상 이익 계산\n",
    "\n",
    "한 에피소드 동안 수집된 각 타임스텝 $t$, ${r_{t}}^{T}*{t=1}$에서 보상의 시퀀스를 예상 이익 ${G*{t}}^{T}_{t=1}$의 시퀀스로 변환합니다. 여기서 보상의 합계는 현재 타임스텝 $t$에서 $T$까지 계산되며, 각 보상에 기하급수적으로 감소하는 할인 계수 $\\gamma$를 곱합니다.\n",
    "\n",
    "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "$\\gamma\\in(0,1)$ 이후, 현재 타임스텝에서 더 멀리 떨어진 보상에는 더 적은 가중치가 부여됩니다.\n",
    "\n",
    "직관적으로, 예상 이익은 단순히 지금 보상이 이후 보상보다 낫다는 것을 암시합니다. 이것은 수학적 의미에서 보상의 합이 수렴하도록 하려는 것입니다.\n",
    "\n",
    "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.835967Z",
     "iopub.status.busy": "2022-12-14T22:10:38.835427Z",
     "iopub.status.idle": "2022-12-14T22:10:38.841321Z",
     "shell.execute_reply": "2022-12-14T22:10:38.840630Z"
    },
    "id": "jpEwFyl315dl"
   },
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhr50_Czxazw"
   },
   "source": [
    "### 3. Actor-Critic 손실\n",
    "\n",
    "하이브리드 Actor-Critic 모델을 사용하고 있기 때문에, 아래와 같이 훈련을 위해 Actor와 Critic 손실의 조합인 손실 함수를 사용합니다.\n",
    "\n",
    "$$L = L_{actor} + L_{critic}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOQIJuG1xdTH"
   },
   "source": [
    "#### Actor 손실\n",
    "\n",
    "[비평가가 상태 종속 기준선인 정책 그래디언트](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s)를 기반으로 행위자 손실을 공식화하고 단일 샘플(에피소드별) 추정치를 계산합니다.\n",
    "\n",
    "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
    "\n",
    "여기서:\n",
    "\n",
    "- $T$: 에피소드별로 달라질 수 있는 에피소드별 타임스텝의 수\n",
    "- $s_{t}$: $t$ 타임스텝의 상태\n",
    "- $a_{t}$: $s$ 상태에 따라 $t$ 타임스텝에서 선택된 동작\n",
    "- $\\pi_{\\theta}$: $\\theta$에 의해 매개변수화된 정책(Actor)\n",
    "- $V^{\\pi}_{\\theta}$: 마찬가지로 $\\theta$에 의해 매개변수화된 값 함수(Critic)\n",
    "- $G = G_{t}$: 주어진 상태에 대한 예상 이익, 타임스텝 $t$에서 동작 쌍\n",
    "\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y304O4OAxiAv"
   },
   "source": [
    "##### 이점\n",
    "\n",
    "$L_{actor}$ 공식에서 $G - V$ 항을 [이점](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions)이라고 하며, 이는 특정한 상태에서 $\\pi$ 정책에 따라 선택된 임의의 동작보다 이 상태에 얼마나 더 나은 동작이 주어지는지를 나타냅니다.\n",
    "\n",
    "기준선을 제외할 수 있지만 이로 인해 훈련 중에 큰 변동이 발생할 수 있습니다. 그리고 비평가 $V$를 기준선으로 선택할 때의 좋은 점은 가능한 한 $G$에 가깝게 훈련되어 변동이 낮아진다는 것입니다.\n",
    "\n",
    "또한, Critic이 없으면 알고리즘이 예상 이익을 바탕으로 특정 상태에서 취하는 행동의 확률을 높이려고 시도할 것이며, 이 때 동작 사이의 상대적 확률이 같게 유지된다면 큰 차이가 생기지 않습니다.\n",
    "\n",
    "예를 들어, 주어진 상태에서 두 행동의 예상 이익이 같다고 가정합니다. Critic이 없으면 알고리즘은 목표 $J$에 따라 이들 동작의 확률을 높이려고 합니다. Critic의 경우, 이점($G - V = 0$)이 없기 때문에 동작의 확률을 높이는 데 따른 이점이 없으며 알고리즘이 그래디언트를 0으로 설정합니다.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hrPLrgGxlvb"
   },
   "source": [
    "#### The Critic loss\n",
    "\n",
    "$V$를 $G$에 최대한 가깝게 훈련하는 것은 다음 손실 함수를 사용한 회귀 문제로 설정할 수 있습니다.\n",
    "\n",
    "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
    "\n",
    "여기서 $L_{\\delta}$는 [Huber 손실](https://en.wikipedia.org/wiki/Huber_loss)로, 제곱 오차 손실보다 데이터의 이상 값에 덜 민감합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.845345Z",
     "iopub.status.busy": "2022-12-14T22:10:38.844751Z",
     "iopub.status.idle": "2022-12-14T22:10:38.849607Z",
     "shell.execute_reply": "2022-12-14T22:10:38.848967Z"
    },
    "id": "9EXwbEez6n9m"
   },
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSYkQOmRfV75"
   },
   "source": [
    "### 4. 매개변수를 업데이트하기 위한 훈련 단계 정의\n",
    "\n",
    "위의 모든 단계를 모든 에피소드에서 실행되는 훈련 단계로 결합합니다. 손실 함수로 이어지는 모든 단계는 `tf.GradientTape` 컨텍스트로 실행되어 자동 미분이 가능합니다.\n",
    "\n",
    "이 튜토리얼에서는 Adam 옵티마이저를 사용하여 모델 매개변수에 그래디언트를 적용합니다.\n",
    "\n",
    "할인되지 않은 보상의 합계인 `episode_reward`도 이 단계에서 계산됩니다. 이 값은 나중에 성공 기준이 충족되는지 평가하는 데 사용됩니다.\n",
    "\n",
    "`tf.function` 컨텍스트를 `train_step` 함수에 적용하여 호출 가능한 TensorFlow 그래프로 컴파일할 수 있고, 그러면 훈련 속도가 10배 빨라질 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.852965Z",
     "iopub.status.busy": "2022-12-14T22:10:38.852688Z",
     "iopub.status.idle": "2022-12-14T22:10:38.864450Z",
     "shell.execute_reply": "2022-12-14T22:10:38.863749Z"
    },
    "id": "QoccrkF3IFCg"
   },
   "outputs": [],
   "source": [
    "\n",
    "#@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode) \n",
    "\n",
    "    # Calculate the expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "    # Calculate the loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFvZiDoAflGK"
   },
   "source": [
    "### 5. 훈련 루프 실행하기\n",
    "\n",
    "성공 기준 또는 최대 에피소드 수에 도달할 때까지 훈련 단계를 실행하는 방식으로 훈련을 실행합니다.\n",
    "\n",
    "대기열을 사용하여 에피소드 보상의 실행 레코드를 유지합니다. 100회 시도에 도달하면 가장 오래된 보상이 대기열의 왼쪽(꼬리쪽) 끝에서 제거되고 최근 보상이 머리쪽(오른쪽)에 추가됩니다. 계산 효율을 높이기 위해 보상의 누적 합계도 유지됩니다.\n",
    "\n",
    "런타임에 따라 훈련은 1분 이내에 완료될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.868193Z",
     "iopub.status.busy": "2022-12-14T22:10:38.867630Z",
     "iopub.status.idle": "2022-12-14T22:17:13.392893Z",
     "shell.execute_reply": "2022-12-14T22:17:13.391873Z"
    },
    "id": "kbmBxnzLiUJx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train(conf, train_df, model):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    env.df = train_df\n",
    "    \n",
    "    min_episodes_criterion = 30\n",
    "    max_episodes = 1000\n",
    "    max_steps_per_episode = 100 #500\n",
    "\n",
    "    # `CartPole-v1` is considered solved if average reward is >= 475 over 500 \n",
    "    # consecutive trials\n",
    "    reward_threshold = 3\n",
    "    running_reward = 0\n",
    "\n",
    "    # The discount factor for future rewards\n",
    "    gamma = 0.99\n",
    "\n",
    "    # Keep the last episodes reward\n",
    "    episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "    t = tqdm.trange(max_episodes)\n",
    "    for i in t:\n",
    "        \n",
    "        n = random.randint(0, len(env.df['date'].values) - max_steps_per_episode - 35)\n",
    "        \n",
    "        conf.start_time = train_df['date'].values[n]\n",
    "        initial_state = env.reset(conf)\n",
    "        #initial_state = tf.constant(initial_state, dtype=tf.float64)\n",
    "        \n",
    "        episode_reward = float(train_step(\n",
    "            initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "        episodes_reward.append(episode_reward)\n",
    "        running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "\n",
    "        t.set_postfix(\n",
    "            episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        # Show the average episode reward every 10 episodes\n",
    "        if i % 10 == 0:\n",
    "          pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "        if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "            break\n",
    "\n",
    "    print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. test\n",
    "\n",
    "test(test dataframe, trained_model)\n",
    "\n",
    " - environment setup\n",
    " - make_rates: 주어진 데이터 범위에서 누적 수익 list 반환\n",
    "\n",
    "make_rates: 주어진 가격 데이터 범위 내의 데이터들에 대한 actor의 각 action들에 의한 reward들을 이용하여 수익률 list 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T22:10:38.825304Z",
     "iopub.status.busy": "2022-12-14T22:10:38.824691Z",
     "iopub.status.idle": "2022-12-14T22:10:38.832122Z",
     "shell.execute_reply": "2022-12-14T22:10:38.831465Z"
    },
    "id": "a4qVRV063Cl9"
   },
   "outputs": [],
   "source": [
    "def make_rates(\n",
    "    state: tf.Tensor,  \n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Runs env step as many as mas steps.\"\"\"\n",
    "\n",
    "    #rates = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    return_df = env.df.loc[35:, ['date', 'close']].reset_index(drop=True)\n",
    "    max_steps = len(return_df)\n",
    "    return_df['rate'] = np.zeros((max_steps,), dtype=float)\n",
    "    return_df['loss_cut'] = np.zeros((max_steps,), dtype=float)\n",
    "    \n",
    "    state_shape = state.shape\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "    \n",
    "        # Convert state into a batched tensor (batch size = 1)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "\n",
    "        # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward = tf_env_step(action)\n",
    "        state.set_shape(state_shape)\n",
    "        \n",
    "        # Store rewards and loss_cut\n",
    "        if t ==0:\n",
    "            return_df.loc[t, 'rate'] = np.array(reward, dtype=float)\n",
    "        else:\n",
    "            return_df.loc[t, 'rate'] = return_df['rate'].values[t-1] + np.array(reward, dtype=float)\n",
    "        return_df.loc[t, 'loss_cut'] = env.loss_cut\n",
    "        \n",
    "    return return_df\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test: \n",
    "주어진 학습 모델과 기간의 dataframe을 이용하여 해당 기간의 누적 수익률 list를 make_rates를 이용하여 반환, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(conf, test_df, model):\n",
    "    \n",
    "    env.df = test_df.reset_index(drop=True)\n",
    "\n",
    "    max_steps = len(test_df) - 35\n",
    "    \n",
    "    initial_state = env.reset(conf)\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float64)\n",
    "\n",
    "    df = make_rates(initial_state, model, max_steps)\n",
    "          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# incrementa learning\n",
    "\n",
    "전체 구간을 1개월 단위로 이동하면서 train, test하여 각 test 결과들을 합쳐서 전체 test 결과를 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 1000/1000 [1:29:15<00:00,  5.36s/it, episode_reward=-5.45, running_reward=-4.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 999: average reward: -4.43!\n",
      "INFO:tensorflow:Assets written to: models/2023-12-31\\assets\n",
      "1.8123583905398846\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_start = ['2021/01/01/09:00', '2021/02/01/09:00', '2021/03/01/09:00', '2021/04/01/09:00', '2021/05/01/09:00',\n",
    "               '2021/06/01/09:00', '2021/07/01/09:00', '2021/08/01/09:00', '2021/09/01/09:00', '2021/10/01/09:00',\n",
    "               '2021/11/01/09:00', '2021/12/01/09:00']\n",
    "\n",
    "train_end = ['2021/12/31/15:00', '2022/01/31/15:00', '2022/02/28/15:00', '2022/03/31/15:00', '2022/04/30/15:00',\n",
    "             '2022/05/31/15:00', '2022/06/30/15:00', '2022/07/31/15:00', '2022/08/31/15:00', '2022/09/30/15:00',\n",
    "             '2022/10/31/15:00', '2022/11/30/15:00']\n",
    "\n",
    "test_end = ['2022/01/31/15:00', '2022/02/28/15:00', '2022/03/31/15:00', '2022/04/30/15:00', '2022/05/31/15:00',\n",
    "            '2022/06/30/15:00', '2022/07/31/15:00', '2022/08/31/15:00', '2022/09/30/15:00', '2022/10/31/15:00',\n",
    "            '2022/11/30/15:00', '2022/12/31/15:00']\n",
    "\"\"\"\n",
    "\n",
    "train_start = ['2021/01/15/09:00']\n",
    "train_end = ['2023/12/31/15:00']\n",
    "test_start = ['2024/01/01/09:00']\n",
    "test_end = ['2024/05/10/15:00']\n",
    "\n",
    "# model 초기화\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "all_return_df = pd.DataFrame(columns=['date', 'close', 'rate'])\n",
    "\n",
    "for i in range(len(train_start)):\n",
    "    # train dataframe 생성\n",
    "    train_df = profit_df.loc[profit_df['date'] >= train_start[i]].reset_index()\n",
    "    train_df = train_df.loc[train_df['date'] <= train_end[i]].reset_index()\n",
    "\n",
    "    # 모델 training\n",
    "    conf.start_time = train_start\n",
    "    conf.end_time = train_end\n",
    "    model = train(conf, train_df, model)\n",
    "\n",
    "    # trained model save\n",
    "    model.save('models/' + train_end[i][:10].replace('/', '-'))\n",
    "    #model.save('models/2023-12-31')\n",
    "\n",
    "    # test dataframe 생성\n",
    "    start_index = profit_df.loc[profit_df['date'] >= test_start[i]].index.min() - 34\n",
    "    end_index = profit_df.loc[profit_df['date'] <= test_end[i]].index.max()\n",
    "    \n",
    "    test_df = profit_df.loc[start_index:end_index].reset_index(drop=True)\n",
    "\n",
    "    # test 후 결과 append\n",
    "    conf.start_time = test_df.loc[0, 'date']\n",
    "    conf.end_time = test_end[i]\n",
    "\n",
    "    \n",
    "    all_return_df = all_return_df.append(test(conf, test_df, model))\n",
    "\n",
    "all_return_df.to_csv('test_results.csv', index=False)\n",
    "    \n",
    "print(all_return_df['rate'].values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnq9Hzo1Po6X"
   },
   "source": [
    "## 다음 단계\n",
    "\n",
    "이 튜토리얼에서는 Tensorflow를 사용하여 Actor-Critic 방법을 구현하는 방법을 보여주었습니다.\n",
    "\n",
    "다음 단계로 Gym의 다른 환경에서 모델의 훈련을 시도할 수 있습니다.\n",
    "\n",
    "Actor-Critic 방법 및 Cartpole-v0 문제에 대한 추가 정보는 다음 리소스를 참조하세요.\n",
    "\n",
    "- [Actor-Critic 메서드](https://hal.inria.fr/hal-00840470/document)\n",
    "- [Actor-Critic 강의(CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
    "- [Cart Pole 학습 제어 문제 [Barto 등 1983]{/a}](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
    "\n",
    "TensorFlow에서 더 많은 강화 학습 예를 보려면 다음 리소스를 확인하세요.\n",
    "\n",
    "- [강화 학습 코드 예제(keras.io)](https://keras.io/examples/rl/)\n",
    "- [TF-Agents 강화 학습 라이브러리](https://www.tensorflow.org/agents)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
